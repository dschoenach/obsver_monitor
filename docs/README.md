# Project Verification and Visualization Suite

## 1. Project Overview

This project provides a suite of tools for the verification and visualization of meteorological forecast data against observations. It allows for the comparison of different forecast experiments, calculation of standard verification metrics (RMSE, Bias, MAE), and the generation of various plots and scorecards to visualize the performance of the forecasts.

The workflow is primarily driven by Python scripts, with a C++ component for high-performance data processing. The outputs are designed to be modular, with metrics stored in Parquet and SQLite formats, which are then used to generate plots and other visualizations.

## 2. Directory Structure

-   `docs/`: Contains this documentation.
-   `out/`: The default output directory for generated metrics, plots, and scorecards.
-   `scr/`: Contains shell scripts for running verification workflows.
-   `src/`: Contains the core Python and C++ source code for verification and plotting.
-   `webapp/`: Contains a PHP-based web application for displaying results.

## 3. Core Components

This section details the main scripts in the `src/` directory.

### `verify.py`

This is the main Python-based verification script. It processes forecast data from SQLite files, compares it against observations, and calculates verification metrics.

-   **Purpose**: To calculate forecast verification statistics (bias, MAE, RMSE) for a given experiment and observation type.
-   **Inputs**:
    -   `--exp-name`: Name of the experiment.
    -   `--data-root`: Path to the directory containing the SQLite database files.
    -   `--obstypevar`: The name of the observation table within the SQLite files.
    -   `--out`: Path for the output Parquet file.
    -   Other options to control grouping, filtering, and parallelism.
-   **Outputs**:
    -   A Parquet file containing the calculated metrics.
    -   Metrics are also written to a `metrics.sqlite` database in the same output directory.

### `verify_cpp_parallel`

This is a high-performance C++ implementation of the verification process. It is designed to be faster than the Python version, especially for large datasets.

-   **Purpose**: To perform the same verification calculations as `verify.py` but with higher performance.
-   **Inputs**: Command-line arguments specifying start/end times, forecast interval, and paths to `vobs` (observation) and `vfld` (forecast) data directories.
-   **Outputs**:
    -   `surface_metrics.csv`: Verification metrics for surface-level observations.
    -   `temp_metrics.csv`: Verification metrics for upper-air (temperature profile) observations.

### `plotting.py`

Generates standard verification plots for a single experiment.

-   **Purpose**: To create time series and vertical profile plots for bias, MAE, and RMSE.
-   **Inputs**:
    -   `--metrics`: A Parquet file generated by `verify.py`.
    -   `--outdir`: Directory to save the plots.
    -   `--title`: A prefix for the plot titles.
-   **Outputs**: PNG images for each metric's time series and profile plot.

### `joint_plotting.py`

Generates comparative plots for multiple experiments.

-   **Purpose**: To create combined time series and vertical profile plots that compare the performance of multiple experiments.
-   **Inputs**:
    -   `--metrics`: A list of Parquet files (one for each experiment).
    -   `--outdir`: Directory to save the plots.
    -   `--title-prefix`: A prefix for plot titles and filenames.
    -   `--exp-color`: Manually assign colors to experiments.
-   **Outputs**: PNG images showing the combined plots.

### `scorecard.py`

Generates a scorecard plot to compare two experiments and assess the statistical significance of the differences in their performance.

-   **Purpose**: To provide a visual summary of the performance difference between two experiments, including statistical significance (Z-score).
-   **Inputs**:
    -   `--exp-a` & `--exp-b`: Names of the two experiments to compare.
    -   `--metrics`: Parquet files containing metrics for both experiments.
    -   `--outdir`: Directory to save the scorecard.
    -   `--title`: Title for the scorecard.
-   **Outputs**:
    -   A PNG image of the scorecard.
    -   A CSV file (`*_zscore_data.csv`) with the underlying data.
    -   Data is also written to the `metrics.sqlite` database in the output directory.

### `build_common_keys.py`

A utility script to find observation keys that are common across multiple experiments. This is useful for ensuring a fair comparison by only evaluating points that are present in all datasets.

-   **Purpose**: To create a file of common observation keys.
-   **Inputs**:
    -   `--exp`: Pairs of experiment names and data root directories.
    -   `--obstypevar`: The observation type to process.
    -   `--out`: The output Parquet file for the common keys.
-   **Output**: A Parquet file containing a single column `obs_key` with the common keys. This file can be used with the `--key-filter` argument in `verify.py`.

### `introspect.py`

A simple utility to inspect the contents of an SQLite database.

-   **Purpose**: To list tables and their columns in a given SQLite file.
-   **Input**: `--db`: Path to the SQLite database file.
-   **Output**: Prints the schema to the console.

## 4. Typical Workflow

1.  **Run Verification**: Use `verify.py` or `verify_cpp_parallel` for each of your forecast experiments to generate metrics from the raw data.
    ```bash
    # Example for one experiment
    python src/verify.py \
        --exp-name "ExperimentA" \
        --data-root /path/to/expA/data \
        --obstypevar "synop" \
        --out out/ExperimentA_synop_metrics.parquet \
        --by-lead
    ```
2.  **Generate Plots**:
    -   For a single experiment, use `plotting.py`.
      ```bash
      python src/plotting.py \
          --metrics out/ExperimentA_synop_metrics.parquet \
          --outdir out/plots/ExperimentA \
          --title "ExperimentA SYNOP"
      ```
    -   For comparing multiple experiments, use `joint_plotting.py`.
      ```bash
      python src/joint_plotting.py \
          --metrics out/ExperimentA_synop_metrics.parquet out/ExperimentB_synop_metrics.parquet \
          --outdir out/plots/Joint \
          --title-prefix "SYNOP_Comparison"
      ```
3.  **Generate Scorecard**: To compare two experiments, use `scorecard.py`.
    ```bash
    python src/scorecard.py \
        --exp-a "ExperimentA" \
        --exp-b "ExperimentB" \
        --metrics out/*.parquet \
        --outdir out/scorecards \
        --title "Scorecard_A_vs_B"
    ```

## 5. Dependencies

### Python

The Python scripts require the following libraries:
-   `polars`
-   `duckdb`
-   `matplotlib`
-   `seaborn`
-   `numpy`

You can install them using pip:
```bash
pip install polars duckdb-engine matplotlib seaborn numpy
```

### C++

The `verify_cpp_parallel` program requires a C++ compiler that supports C++17 and OpenMP. To compile it:
```bash
g++ -std=c++17 -fopenmp -o src/verify_cpp_parallel src/verify_cpp_parallel.cpp
```
